{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luisARC18/DS_Online_Mayo24_Exercises/blob/main/13_Ejercicios_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L58Wwdikgnmf"
      },
      "source": [
        "![image.png](attachment:2d551f07-161e-4fb5-8475-4602b8dfe4ad.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tZpxPhIgnmi"
      },
      "source": [
        "![image.png](attachment:c02d1fc9-343b-419b-b3c2-1a75cb0a6a71.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W1PBw61gnmj"
      },
      "source": [
        "Para ejercitarte y afianzar lo aprendido sobre **Embeddings y procesamiento de texto**, completa los siguientes ejercicios. Recuerda que necesitarás datos que están en el directorio data que acompaña al notebook (búscalo en el repositorio de ejercicios).\n",
        "\n",
        "La solución a los mismos las tienes ya, y en este caso se te invita a QUE SIGAS EL EJERCICIO CON LA SOLUCION a modo de tutorial ya que hay varios aspectos que son nuevos y se introducen en la solución."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZUQErGewZxE"
      },
      "source": [
        "### Ejercicio 0\n",
        "\n",
        "Importa las librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kVCPng9_gnml"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBFctV8-JZOc"
      },
      "source": [
        "### Ejercicio 1: Descarga el dataset\n",
        "\n",
        "Usarás el [Conjunto de Datos de Grandes Reseñas de Películas](http://ai.stanford.edu/~amaas/data/sentiment/) a lo largo del tutorial. Entrenarás un modelo de clasificador de sentimientos con este conjunto de datos y, en el proceso, aprenderás embeddings desde cero.\n",
        "\n",
        "Descarga el conjunto de datos utilizando la utilidad de archivos de Keras y echa un vistazo a los directorios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKD4MNvignmn",
        "outputId": "57ed6c2e-a71e-424c-c2a3-c9843b0d6b84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "\u001b[1m84125825/84125825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", origin = 'file:/./aclImdb_v1.tar.gz',\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')"
      ],
      "metadata": {
        "id": "Iz-YF2Q_hPWj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNcD41Ylhao4",
        "outputId": "44631868-50a4-4358-e308-12f8eaefc944"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./aclImdb_v1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUntPYgChaUa",
        "outputId": "cc21a59c-7e72-4585-d953-06d8e6481e14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOZ0Y6_Ngnmo"
      },
      "source": [
        "### Ejercicio 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY6yROZNKvbd"
      },
      "source": [
        "Echa un vistazo al directorio train/. Tiene carpetas pos y neg con reseñas de películas etiquetadas como positivas y negativas respectivamente. Utilizarás reseñas de las carpetas pos y neg para entrenar un modelo de clasificación binaria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mIg5wuFgnmo",
        "outputId": "425716aa-d8f2-47c7-d295-fa4a8abc26b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['unsup',\n",
              " 'labeledBow.feat',\n",
              " 'pos',\n",
              " 'urls_pos.txt',\n",
              " 'unsupBow.feat',\n",
              " 'urls_neg.txt',\n",
              " 'urls_unsup.txt',\n",
              " 'neg']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dataset_dir = \"./aclImdb\"\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GHgVO-Rgnmo"
      },
      "source": [
        "### Ejercicio 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O59BdioK8jY"
      },
      "source": [
        "El directorio `train` también tiene carpetas adicionales que deberían ser eliminadas antes de crear el conjunto de datos de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ll7bTlw6gnmp"
      },
      "outputs": [],
      "source": [
        "shutil.rmtree(dataset_dir + \"/train/unsup\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mQOcl3-gnmp"
      },
      "source": [
        "### Ejercicio 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFoJjiEyJz9u"
      },
      "source": [
        "A continuación, crea un tf.data.Dataset usando tf.keras.preprocessing.text_dataset_from_directory. Puedes leer más sobre cómo utilizar esta utilidad en este [tutorial de clasificación de texto](https://www.tensorflow.org/tutorials/keras/text_classification).\n",
        "\n",
        "Usa el directorio de entrenamiento para crear conjuntos de datos de entrenamiento y validación con una división del 20% para la validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t69rQDlgnmp",
        "outputId": "1c79982c-34bd-46b9-b7da-af4beb2d5915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1024\n",
        "seed = 123\n",
        "\n",
        "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHV2pchDhzDn"
      },
      "source": [
        "### Ejercicio 5: Configura el dataset para mejorar el rendimiento (mira la solución)\n",
        "\n",
        "Estos son dos métodos importantes que deberías usar al cargar datos para asegurarte de que las operaciones de entrada/salida no se conviertan en un bloqueo.\n",
        "\n",
        "`.cache()` mantiene los datos en memoria después de ser cargados desde el disco. Esto garantizará que el conjunto de datos no se convierta en un cuello de botella mientras entrenas tu modelo. Si tu conjunto de datos es demasiado grande para caber en la memoria, también puedes usar este método para crear una caché en disco eficiente, la cual es más eficiente para leer que muchos archivos pequeños.\n",
        "\n",
        "`.prefetch()` solapa la preprocesación de datos y la ejecución del modelo durante el entrenamiento.\n",
        "\n",
        "Puedes aprender más sobre ambos métodos, así como cómo cachear datos en disco en la [guía de rendimiento de datos](https://www.tensorflow.org/guide/data_performance).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EvWTyVfJgnmp"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqBazMiVQkj1"
      },
      "source": [
        "### Ejercicio 6: Repasando Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwO6dSU3gnmq"
      },
      "source": [
        "Crea una capa de Embedding con dimensión de entrada 1000 y dimensión de salida 4. Codifica las frases \"Me llamo Iñigo Montoya\", \"Tú mataste a mi padre\", \"Disponte a morir\" (tendrás que crear una capa adicional). Haz la codificación de cada una por separado y luego prueba a ponerlas todas juntas en una misma lista. Ojo, cada frase la tienes que convertir a una lista de palabras. ¿Qué ocurre en este último caso? [Haz el ejercicio y luego mira la solución]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWVWg-yKgnmq",
        "outputId": "574c23c2-1357-4aaf-dffd-e9e9ac3efe39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Para <Me llamo Iñigo Montoya>, embeddings: [[[-0.03555409 -0.04887104 -0.02318512  0.03655957]\n",
            "  [-0.02649965  0.03554846 -0.02570039 -0.03923801]\n",
            "  [-0.01435436  0.04216019  0.03296575  0.04677923]\n",
            "  [ 0.04929413 -0.04090649  0.00119938 -0.04651868]]]\n",
            "Para <Tú mataste a mi padre>, embeddings: [[[-0.04412226  0.0297466   0.04155841 -0.00410807]\n",
            "  [ 0.01915077 -0.0315034   0.0003183  -0.0166335 ]\n",
            "  [ 0.03670141  0.00561271  0.02508364 -0.02874237]\n",
            "  [ 0.02587185  0.02433972  0.0364666   0.01616457]\n",
            "  [ 0.01801965  0.04099276  0.02250893 -0.03725059]]]\n",
            "Para <Disponte a morir>, embeddings: [[[ 0.04504815  0.01959893  0.02982488  0.0182672 ]\n",
            "  [ 0.03670141  0.00561271  0.02508364 -0.02874237]\n",
            "  [-0.0167987  -0.02015652  0.01680411 -0.04538568]]]\n"
          ]
        }
      ],
      "source": [
        "frases = [\"Me llamo Iñigo Montoya\", \"Tú mataste a mi padre\", \"Disponte a morir\"]\n",
        "\n",
        "layer_test = Embedding(input_dim=1000, output_dim= 4)\n",
        "pre_procesado = tf.keras.layers.StringLookup()\n",
        "pre_procesado.adapt(\" \".join(frases).split())\n",
        "conversor_fake = tf.keras.models.Sequential(\n",
        "    [pre_procesado,\n",
        "     layer_test])\n",
        "resultados = []\n",
        "for frase in frases:\n",
        "    resultados.append(conversor_fake(tf.constant([frase.split()])))\n",
        "    print(f\"Para <{frase}>, embeddings: {resultados[-1].numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGicgV5qT0wh"
      },
      "source": [
        "### Ejercicio 7: Procesado de Texto con vectorización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6NZSqIIoU0Y"
      },
      "source": [
        "A continuación, define los pasos de preprocesamiento del conjunto de datos necesarios para tu modelo de clasificación de sentimientos. Inicializa una capa de TextVectorization con los parámetros deseados para vectorizar las reseñas de películas. Recuerda que tendrás que limipiar (tambien se llama \"estandandizar\") las reseñas (como lo hicimos en su día o como en el workout o como lo hacemos, diferente, en la solución).\n",
        "\n",
        "Importante: En este ejercicio vamos a usar la capa de vectorización pero no para convertir las frases en conteos de palabras o sus tfidf, sino en listas de indices en un vocabulario. Por eso a la hora de inicializar la capa debes usar el output_mode = \"int\" (en el workout lo hemos empleado con \"count\" y con \"tf-idf\". Configura la capa de forma que ajuste su salida a secuencias o frases de 8 palabras.\n",
        "\n",
        "Prueba la nueva capa con las frases del ejercicio anterior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FQHVakRKgnmq"
      },
      "outputs": [],
      "source": [
        "def custom_cleaner(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "    return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "\n",
        "sequence_length = 8\n",
        "\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_cleaner,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.adapt(frases)"
      ],
      "metadata": {
        "id": "fnRr-VekiW7O"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer(frases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr9WlzMZiaob",
        "outputId": "5faffae4-0461-4288-ad49-a2f965f876ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 8), dtype=int64, numpy=\n",
              "array([[ 8, 10, 11,  6,  0,  0,  0,  0],\n",
              "       [ 3,  9,  2,  7,  4,  0,  0,  0],\n",
              "       [12,  2,  5,  0,  0,  0,  0,  0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for indice,word in enumerate(vectorize_layer.get_vocabulary()):\n",
        "    print(\"%d -> %s\" %(indice,word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyIZYPjHiafu",
        "outputId": "25cd58d3-3dc6-4fbf-b1c2-b89a4b3d2a99"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -> \n",
            "1 -> [UNK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx3h22DUgnmq"
      },
      "source": [
        "### Ejercicio 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfzW7ialgnmq"
      },
      "source": [
        "Recrea la capa del ejercicio anterior, llamándola vectorizer_layer. pero esta vez para un vocabulario de 1000 terminos y para un tamaño de secuencia de 100 palabras. LUEGO ejecuta el código que tienes en la siguiente celda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NJeI1T0Jgnmq"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "    return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "vocab_size = 10000\n",
        "sequence_length = 100\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNQjD4gIgnmr"
      },
      "source": [
        "**Para ejecutar después de la creación de la capa vectorize_layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BIm9nV9Kgnmr"
      },
      "outputs": [],
      "source": [
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "# Nos quitamos los labels de esta manera, que van en conjunto con features en train_ds\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0xUZumkgnmr"
      },
      "source": [
        "### Ejercicio 9\n",
        "\n",
        "Es hora de crear el modelo de clasificación. Tendrás que emplear una capa [\"GlobalAveragePooling1D\"](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) e investigar por tu cuenta un poco sobre ella, aunque aquí te dejo algunos apuntes. En definitiva, tu modelo tiene que tener:\n",
        "\n",
        "* La capa [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) transforma cadenas en índices de vocabulario. Ya has inicializado `vectorize_layer` como una capa de TextVectorization y has construido su vocabulario llamando a `adapt` en `text_ds`. Ahora, vectorize_layer puede ser utilizada como la primera capa de tu modelo de clasificación de principio a fin, alimentando cadenas transformadas en la capa de Embedding. Utiliza una dimensión de salida de 16 en el embedding.\n",
        "  \n",
        "* La capa [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) toma el vocabulario codificado en enteros y busca el vector de incrustación para cada índice de palabra. Estos vectores se aprenden a medida que el modelo se entrena. Los vectores añaden una dimensión al arreglo de salida. Las dimensiones resultantes son: `(lote, secuencia, incrustación)`.\n",
        "\n",
        "* La capa [`GlobalAveragePooling1D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) devuelve un vector de salida de longitud fija para cada ejemplo promediando sobre la dimensión de secuencia. Esto permite al modelo manejar entrada de longitud variable, de la manera más simple posible. Esta capa hace el sentence embedding que comentamos en el workout, es decir convierte la sentencia en un embedding que es el resultado de hacer la media de cada uno de sus word embeddings (sí es el centroide de sus embeddings) ya que es la forma más sencilla de hacer sentence embedding.\n",
        "\n",
        "* El vector de salida de longitud fija se pasa a través de una capa completamente conectada ([`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) con 16 unidades ocultas. (porque tenemos 16 de dimensión del embedding de palabra y de sentencia).\n",
        "\n",
        "* La última capa está densamente conectada con un único nodo de salida (por ser un clasificador binario).\n",
        "\n",
        "Precaución: Este modelo no utiliza enmascaramiento, por lo que el relleno de ceros se utiliza como parte de la entrada y, por lo tanto, la longitud del relleno puede afectar la salida. Para solucionar esto, consulta la [guía de enmascaramiento y relleno](https://www.tensorflow.org/guide/keras/masking_and_padding). Básicamente, al hacer el sentence embedding en frases cortas (imaginate una de una sola palabra, tendrá en nuestro caso 1 word-embedding y 99 de relleno) el embedding de la sentencia se ve completamente sesgado hacia el de relleno, por eso se usa el masking para decirle a la capa que sólo tenga en cuenta el embedding que no sea de relleno.y relleno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "2JL3kGZLgnmr",
        "outputId": "bfca0cae-ea4d-472d-deed-d50b93d35167"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-23-a45d259b2e4f>, line 16)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-a45d259b2e4f>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    Dense(1, activation = \"sigmoid\") # originalmente no tiene activacion\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "embedding_dim=16\n",
        "\n",
        "'''\n",
        "GlobalAveragePooling1D\n",
        "Cada palabra tiene asociado un embedding. El ouput es la media de cada\n",
        "coordenada del embedding, por tanto, si hay 16 embeddings, hará un\n",
        "flatten a 16, siendo cada valor la media de la coordenada de ese\n",
        "embedding para todas las palabras de la review\n",
        "'''\n",
        "\n",
        "model = Sequential([\n",
        "  vectorize_layer, # 100 [1, 3, 4, 4, 90, ...]\n",
        "  Embedding(vocab_size, embedding_dim, name=\"embedding\"), # 10.000 x 16 --> [[], [], [] ...] 100x16\n",
        "  GlobalAveragePooling1D(), # [] 16\n",
        "  Dense(16, activation='relu'), #\n",
        "  Dense(1, activation = \"sigmoid\") # originalmente no tiene activacion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjLNgKO7W2fe"
      },
      "source": [
        "### Ejercicio 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OrKAKAKIbuH"
      },
      "source": [
        "Entrena el modelo usando un optimizador \"Adam\" y la función de perdida \"BinaryCrossEntropy\". Evalua contra test. Prueba con 15 épocas (es muy pesado el entrenamiento)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "TcTztUL_gnms",
        "outputId": "d686afda-d683-4294-9b21-4de20f6b197e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0648f2cf1052>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.compile(optimizer='adam',\n\u001b[0m\u001b[1;32m      2\u001b[0m               \u001b[0;31m# binary_crossentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0;31m# https://stackoverflow.com/questions/61233425/what-should-i-use-as-target-vector-when-i-use-binarycrossentropyfrom-logits-tru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               metrics=['accuracy'])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              # binary_crossentropy\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              # https://stackoverflow.com/questions/61233425/what-should-i-use-as-target-vector-when-i-use-binarycrossentropyfrom-logits-tru\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyMNA5MQgnms"
      },
      "source": [
        "### Bonus: Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaPnNQdlgnmt"
      },
      "source": [
        "Cuando hayas completado los ejercicios, lee este apartado de la solución."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}